{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve for $\\theta$\n",
    "$$y = X\\theta$$\n",
    "\n",
    "**Direct inverse**:\n",
    "$$ \\theta = X^{-1}y $$\n",
    "Only works for square X, X may not be invertible, matrix inversion is slow and more making it useless in practice.\n",
    "\n",
    "--------------\n",
    "\n",
    "**Analytic solution**:\n",
    "$$ \\theta = (X^TX)^{-1}X^Ty $$\n",
    "\n",
    "This can be derived by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X\\theta & = y \\\\\n",
    "X^TX\\theta & = X^Ty \\\\\n",
    "\\theta & = (X^TX)^{-1}X^Ty \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$ (X^TX)^{-1}X^T $ is equivalent to the Moore-Penrose pseudo-inverse.\n",
    "`pinv` from the LinearAlgebra module provides this. \n",
    "\n",
    "A better \"machine learning\" proof by minimising the MSE cost function\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "              J(\\theta) & = ||y - X\\theta||_2 \\\\\n",
    "\\nabla_\\theta J(\\theta) & = \\nabla_\\theta\\frac{1}{2}(X\\theta - y)^T(X\\theta - y)\\\\\n",
    "                        & = \\nabla_\\theta\\frac{1}{2}(\\theta^TX^T - y^T)(X\\theta - y)\\\\\n",
    "                        & = \\nabla_\\theta\\frac{1}{2}(\\theta^TX^TX\\theta - \\theta^TX^Ty - y^TX\\theta + y^Ty)\\\\\n",
    "                        & = \\nabla_\\theta\\frac{1}{2}(\\theta^TX^TX\\theta - 2\\theta^TX^Ty + y^Ty)\\\\\n",
    "                        & = \\frac{1}{2}(2X^TX\\theta - 2X^Ty)\\\\\n",
    "                        & = X^TX\\theta - X^Ty\\\\\n",
    "             X^TX\\theta & = X^Ty\\\\\n",
    "                 \\theta & = (X^TX)^{-1}X^Ty\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Notes:\n",
    "* $\\theta^TX^Ty$ and $y^TX\\theta$ are are transposes of each other. They are also scalar valued (check the dimensions), so we can just add them up.\n",
    "* The $ \\frac{1}{2} $ is just added for convenience later on.\n",
    "* $\\nabla_\\theta$ means the matrix derivative with respect to $\\theta$\n",
    "* Matrix differentiation rules:\n",
    "$$\n",
    "\\begin{align}\n",
    "x^TB  & \\rightarrow B\\\\\n",
    "x^Tb  & \\rightarrow b\\\\\n",
    "x^Tx  & \\rightarrow 2x\\\\\n",
    "x^TBx & \\rightarrow 2Bx\\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# θ = [5 2]\n",
    "f(x) = 5x .+ 2\n",
    "x = rand(2 ^ 12)\n",
    "y = f.(x);\n",
    "\n",
    "# include bias term\n",
    "X = hcat(x, ones(size(x)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "analyticsolution (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function analyticsolution(X, y)\n",
    "    # pinv(X) * y\n",
    "    inv(X' * X) * X' * y \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 5.00000000000001\n",
       " 1.999999999999999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyticsolution(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**\n",
    "\n",
    "It's defined as\n",
    "$$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta) $$\n",
    "This just means adjust $\\theta$ with respect to how much error there was.\n",
    "\n",
    "For linear regression, we have to find out the partial derivative of the cost function with respect to each feature.\n",
    "A mean squared error is an intuitive cost function to use.\n",
    "\n",
    "Assuming there's only one training example\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\theta_j}J(\\theta) & = \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2}(h_\\theta(x) - y)^2\\\\\n",
    "                                            & = 2 \\cdot \\frac{1}{2} (h_\\theta - y) \\frac{\\partial}{\\partial \\theta_j} (h_\\theta(x) - y)\\\\\n",
    "                                            & = (h_\\theta - y) \\frac{\\partial}{\\partial \\theta_j} (\\sum \\theta x - y)\\\\\n",
    "                                            & = (h_\\theta - y) \\cdot x_j\\\\\n",
    "\\end{align}\\\\\n",
    "$$\n",
    "\n",
    "Plugging it into the gradient descent formula\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\alpha (h_\\theta - y) \\cdot x_j $$\n",
    "\n",
    "For more than one training example\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\sum \\alpha (h_\\theta - y) \\cdot x_j $$\n",
    "\n",
    "\n",
    "Notes:\n",
    "* Apply chain rule\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradientdescent (generic function with 3 methods)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function h(x, θ)\n",
    "    x' * θ\n",
    "end\n",
    "\n",
    "function gradientdescent(X, y, α=0.01, tol=1e-20)\n",
    "    θ = rand(size(X, 2))\n",
    "    prev = θ\n",
    "    diff = +Inf\n",
    "    while diff > tol\n",
    "        for i = 1:size(X)[1]\n",
    "            θ += α * (y[i] - sum(h(X[i, :], θ))) .* X[i, :]\n",
    "        end\n",
    "        diff = sum((prev .- θ) .^ 2)\n",
    "        prev = θ\n",
    "    end\n",
    "    θ\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 4.9999999999995195\n",
       " 2.000000000000256"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientdescent(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "\n",
    "**QR Decomposition**\n",
    "\n",
    "The QR decomposition factorizes a matrix $A$ into\n",
    "* $Q$ an orthogonal matrix\n",
    "* $R$ an upper triangular matrix.\n",
    "\n",
    "Orthonormal matrices have a nice property:\n",
    "$Q^TQ = QQ^T = I$ and $Q^T = Q^{-1}$\n",
    "\n",
    "There are many ways of computing the QR decomposition. Some are\n",
    "* Gram-Schmidt process\n",
    "* Householder reflections\n",
    "* Givens rotations\n",
    "\n",
    "In the case of linear regression:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X\\theta & = y\\\\\n",
    "QR\\theta & = y\\\\\n",
    "Q^T QR & = Q^T y\\\\\n",
    "(I)R & = Q^T y\n",
    "\\end{align}\n",
    "$$\n",
    "As $R$ is an upper triangular matrix, we can easily solve for $y$ using back-substitution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qr_least_squares (generic function with 1 method)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# project v onto u\n",
    "function project(u::AbstractVector, v::AbstractVector)\n",
    "    ((v ⋅ u) / (u ⋅ u)) .* u\n",
    "end\n",
    "\n",
    "\n",
    "function gram_schmidt(A::AbstractMatrix)\n",
    "    X = similar(A)\n",
    "    X[:, 1] = A[:, 1] / norm(A[:, 1])\n",
    "    for i = 2:size(X, 2)\n",
    "        X[:, i] = A[:, i]\n",
    "        for j = 1:i-1\n",
    "            X[:, i] -= project(X[:, j], X[:, i])\n",
    "        end\n",
    "        X[:, i] /= norm(X[:, i])\n",
    "    end\n",
    "    X\n",
    "end\n",
    "\n",
    "function backward_substitution(lhs::UpperTriangular, rhs::AbstractVector)\n",
    "    rows, cols = size(lhs)\n",
    "    solution = ones(cols)\n",
    "    solution[rows] = rhs[rows] / lhs[rows, cols]\n",
    "    for i = rows-1:-1:1\n",
    "        solution[i] = (rhs[i] - lhs[i, i+1:end] ⋅ solution[i+1:end]) / lhs[i, i]\n",
    "    end\n",
    "    solution\n",
    "end\n",
    "\n",
    "function qr_least_squares(X::AbstractMatrix, y::AbstractVector)\n",
    "    Q = gram_schmidt(X)\n",
    "    R = UpperTriangular(Q'X)\n",
    "    backward_substitution(R, vec(Q'y))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 5.000000000000006\n",
       " 1.9999999999999991"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qr_least_squares(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "Note that the above are just for showing how it can be computed, the easiest way to solve linear equations is in Julia is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 5.000000000000001\n",
       " 1.9999999999999987"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X \\ y"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
